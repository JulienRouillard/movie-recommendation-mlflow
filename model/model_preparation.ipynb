{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df3a4d3",
   "metadata": {},
   "source": [
    "# MovieLens Recommandation System - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7763cda2",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bbda8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09db7a5",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72cf173",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24bf5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¥ Loading data from Neon database...\n",
      "  - Loading ratings table...\n",
      "  - Loading movies table...\n",
      "\n",
      "âœ… Data loaded successfully!\n",
      "  - Ratings: 1,000,000 interactions\n",
      "  - Movies: 62,423 films\n",
      "  - Users: 9,189 unique users\n",
      "\n",
      "ðŸ” Ratings dataset:\n",
      "   userId  movieId  rating   timestamp            datetime  year  liked  \\\n",
      "0  110286   109388     3.5  1494693096 2017-05-13 16:31:36  2017      1   \n",
      "1  112119      223     4.0  1494693125 2017-05-13 16:32:05  2017      1   \n",
      "2  112119    76251     4.0  1494693135 2017-05-13 16:32:15  2017      1   \n",
      "3  112119    84152     4.0  1494693136 2017-05-13 16:32:16  2017      1   \n",
      "4  112119    61024     3.0  1494693145 2017-05-13 16:32:25  2017      0   \n",
      "\n",
      "                             title               genres data_split  \\\n",
      "0  All the Light in the Sky (2012)                Drama      train   \n",
      "1                    Clerks (1994)               Comedy      train   \n",
      "2                  Kick-Ass (2010)        Action|Comedy      train   \n",
      "3                 Limitless (2011)      Sci-Fi|Thriller      train   \n",
      "4         Pineapple Express (2008)  Action|Comedy|Crime      train   \n",
      "\n",
      "                   loaded_at  \n",
      "0 2025-12-17 14:22:11.675708  \n",
      "1 2025-12-17 14:22:11.675708  \n",
      "2 2025-12-17 14:22:11.675708  \n",
      "3 2025-12-17 14:22:11.675708  \n",
      "4 2025-12-17 14:22:11.675708  \n",
      "\n",
      "Columns: ['userId', 'movieId', 'rating', 'timestamp', 'datetime', 'year', 'liked', 'title', 'genres', 'data_split', 'loaded_at']\n",
      "Memory usage: 272.05 MB\n",
      "\n",
      "ðŸŽ¬ Movies dataset:\n",
      "   movieId                               title  (no genres listed)  Action  \\\n",
      "0        1                    Toy Story (1995)                   0       0   \n",
      "1        2                      Jumanji (1995)                   0       0   \n",
      "2        3             Grumpier Old Men (1995)                   0       0   \n",
      "3        4            Waiting to Exhale (1995)                   0       0   \n",
      "4        5  Father of the Bride Part II (1995)                   0       0   \n",
      "\n",
      "   Adventure  Animation  Children  Comedy  Crime  Documentary  Drama  Fantasy  \\\n",
      "0          1          1         1       1      0            0      0        1   \n",
      "1          1          0         1       0      0            0      0        1   \n",
      "2          0          0         0       1      0            0      0        0   \n",
      "3          0          0         0       1      0            0      1        0   \n",
      "4          0          0         0       1      0            0      0        0   \n",
      "\n",
      "   Film-Noir  Horror  IMAX  Musical  Mystery  Romance  Sci-Fi  Thriller  War  \\\n",
      "0          0       0     0        0        0        0       0         0    0   \n",
      "1          0       0     0        0        0        0       0         0    0   \n",
      "2          0       0     0        0        0        1       0         0    0   \n",
      "3          0       0     0        0        0        1       0         0    0   \n",
      "4          0       0     0        0        0        0       0         0    0   \n",
      "\n",
      "   Western   g_emb_0   g_emb_1   g_emb_2   g_emb_3   g_emb_4   g_emb_5  \\\n",
      "0        0  5.691517  1.157252 -2.299336  0.432646  1.473964  1.740654   \n",
      "1        0  3.842368  2.398970 -1.019806  0.010039  1.318514  0.216763   \n",
      "2        0  2.881668  1.250843 -0.641893 -0.263697 -0.496168  0.352696   \n",
      "3        0  2.781194  0.691663 -1.035577 -0.691427 -0.678014 -0.502191   \n",
      "4        0  2.862795  1.388638 -1.272278 -0.620198 -0.514246  0.082961   \n",
      "\n",
      "    g_emb_6   g_emb_7   g_emb_8   g_emb_9  g_emb_10  g_emb_11  g_emb_12  \\\n",
      "0  0.675731  1.929917 -0.530810  0.824875 -1.228916 -0.212062 -0.595200   \n",
      "1  0.049387  1.165664  0.326260  0.383432 -0.815279 -0.079969 -0.760628   \n",
      "2 -0.112907 -0.016498  0.080669  0.033166  0.100299 -0.165580  0.546892   \n",
      "3 -0.735465  0.146584  0.280361  0.103981  0.561558 -0.236180 -0.071159   \n",
      "4 -0.343221  0.313066  0.034632  0.131050 -0.124427 -0.444375  0.805534   \n",
      "\n",
      "   g_emb_13  g_emb_14  g_emb_15  g_emb_16  g_emb_17  g_emb_18  g_emb_19  \\\n",
      "0 -0.779059 -0.869604 -0.701330 -0.979965 -0.131128  0.134645  0.138563   \n",
      "1 -0.445202  0.011121 -0.018436  0.309645  0.294101  0.391970 -0.191418   \n",
      "2 -0.053782  0.034524 -0.178806 -0.160523  0.008706  0.068131 -0.170240   \n",
      "3 -0.402102 -0.021885  0.223924 -0.365369  0.461240  0.125611  0.223589   \n",
      "4 -0.441343 -0.266757  0.002138 -0.160063 -0.063511  0.154974 -0.119219   \n",
      "\n",
      "   g_emb_20  g_emb_21  g_emb_22  g_emb_23  g_emb_24  g_emb_25  g_emb_26  \\\n",
      "0  0.360492 -0.204207 -0.124961  0.674358 -0.434846  0.515796  0.189126   \n",
      "1  0.055164 -0.185157 -0.636815  0.245848 -0.086156 -0.223896 -0.246100   \n",
      "2 -0.129929 -0.354337 -0.218133  0.527261  0.200836 -0.397738  0.940024   \n",
      "3 -0.361044 -0.234253  0.153604 -0.012468  0.102460 -0.070176 -0.045276   \n",
      "4 -0.072274 -0.201397 -0.236796  0.094672  0.204094 -0.746639  0.779511   \n",
      "\n",
      "   g_emb_27  g_emb_28  g_emb_29  g_emb_30  g_emb_31  g_emb_32  g_emb_33  \\\n",
      "0  0.082785 -0.428867  0.257976 -0.101714 -0.044729  0.106466 -0.188712   \n",
      "1  0.586553 -0.661727  0.016247 -0.409609  0.494764  0.049024  0.430545   \n",
      "2 -0.093480  0.571489  0.147972 -0.160032 -0.078550  0.334640 -0.129780   \n",
      "3 -0.072748  0.122920  0.087131  0.147699  0.125393 -0.046573 -0.149079   \n",
      "4  0.151690  0.656710 -0.288529 -0.290381  0.372024  0.684369 -0.302088   \n",
      "\n",
      "   g_emb_34  g_emb_35  g_emb_36  g_emb_37  g_emb_38  g_emb_39  g_emb_40  \\\n",
      "0  0.090161  0.199651  0.490063  0.163367 -0.201282 -0.262652 -0.140062   \n",
      "1  0.111794 -0.221712 -0.101372  0.123574 -0.181107 -0.009608  0.276904   \n",
      "2 -0.202380  0.119650 -0.272003  0.123709  0.311968  0.198051 -0.108482   \n",
      "3 -0.044108  0.113031 -0.006297  0.121644 -0.020657  0.009819 -0.384116   \n",
      "4 -0.292647  0.161840 -0.233127  0.091379  0.398576  0.186328 -0.229169   \n",
      "\n",
      "   g_emb_41  g_emb_42  g_emb_43  g_emb_44  g_emb_45  g_emb_46  g_emb_47  \\\n",
      "0  0.020007 -0.185655  0.042601  0.129450  0.336696  0.080704  0.085826   \n",
      "1 -0.069656 -0.106094  0.349968 -0.074469 -0.015481  0.002498  0.150418   \n",
      "2 -0.248382  0.008225 -0.332304  0.009207 -0.172700 -0.031802  0.016577   \n",
      "3 -0.093480  0.082265  0.139093 -0.211835  0.007253 -0.142058 -0.174887   \n",
      "4 -0.220883 -0.181569 -0.239166 -0.100677 -0.152358 -0.223660  0.185833   \n",
      "\n",
      "   g_emb_48  g_emb_49  g_emb_50  g_emb_51  g_emb_52  g_emb_53  g_emb_54  \\\n",
      "0  0.039391 -0.084068  0.213770 -0.242639 -0.146221  0.054556 -0.051027   \n",
      "1 -0.077823  0.238162 -0.380450 -0.140034 -0.035211  0.429892  0.080299   \n",
      "2 -0.256653  0.189994 -0.048918  0.268648 -0.128206  0.159127 -0.003522   \n",
      "3 -0.224110 -0.123165  0.228665 -0.023693  0.363432  0.032839 -0.316506   \n",
      "4 -0.127749  0.047804  0.146478  0.246765 -0.060582  0.126789  0.262960   \n",
      "\n",
      "   g_emb_55  g_emb_56  g_emb_57  g_emb_58  g_emb_59  g_emb_60  g_emb_61  \\\n",
      "0  0.099649 -0.244833  0.003951 -0.136712  0.325241  0.071950  0.151897   \n",
      "1  0.077760 -0.001979 -0.169908 -0.328370 -0.155115 -0.324047  0.229150   \n",
      "2 -0.255299 -0.047662  0.043895  0.025116 -0.006282  0.120842  0.127993   \n",
      "3  0.234975  0.107907  0.136268  0.067956  0.319556  0.008732  0.139272   \n",
      "4 -0.265437  0.022199  0.152880  0.167350  0.148737  0.113985  0.046422   \n",
      "\n",
      "   g_emb_62  g_emb_63  \n",
      "0 -0.005155 -0.211419  \n",
      "1  0.065371  0.075309  \n",
      "2 -0.087254 -0.039121  \n",
      "3 -0.150812  0.039239  \n",
      "4 -0.158531  0.106722  \n",
      "\n",
      "Columns: ['movieId', 'title', '(no genres listed)', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'IMAX', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western', 'g_emb_0', 'g_emb_1', 'g_emb_2', 'g_emb_3', 'g_emb_4', 'g_emb_5', 'g_emb_6', 'g_emb_7', 'g_emb_8', 'g_emb_9', 'g_emb_10', 'g_emb_11', 'g_emb_12', 'g_emb_13', 'g_emb_14', 'g_emb_15', 'g_emb_16', 'g_emb_17', 'g_emb_18', 'g_emb_19', 'g_emb_20', 'g_emb_21', 'g_emb_22', 'g_emb_23', 'g_emb_24', 'g_emb_25', 'g_emb_26', 'g_emb_27', 'g_emb_28', 'g_emb_29', 'g_emb_30', 'g_emb_31', 'g_emb_32', 'g_emb_33', 'g_emb_34', 'g_emb_35', 'g_emb_36', 'g_emb_37', 'g_emb_38', 'g_emb_39', 'g_emb_40', 'g_emb_41', 'g_emb_42', 'g_emb_43', 'g_emb_44', 'g_emb_45', 'g_emb_46', 'g_emb_47', 'g_emb_48', 'g_emb_49', 'g_emb_50', 'g_emb_51', 'g_emb_52', 'g_emb_53', 'g_emb_54', 'g_emb_55', 'g_emb_56', 'g_emb_57', 'g_emb_58', 'g_emb_59', 'g_emb_60', 'g_emb_61', 'g_emb_62', 'g_emb_63']\n"
     ]
    }
   ],
   "source": [
    "NEON_CONNECTION_STRING = \"postgresql://neondb_owner:npg_s5NhbHAIkE3W@ep-square-truth-agcbtrap.c-2.eu-central-1.aws.neon.tech/neondb?sslmode=require\"\n",
    "\n",
    "# Load data from Neon\n",
    "print(\"\\nðŸ“¥ Loading data from Neon database...\")\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(NEON_CONNECTION_STRING)\n",
    "\n",
    "# Load ratings from database\n",
    "print(\"  - Loading ratings table...\")\n",
    "ratings = pd.read_sql(\"SELECT * FROM ratings\", engine)\n",
    "\n",
    "# Load movies from database\n",
    "print(\"  - Loading movies table...\")\n",
    "movies = pd.read_sql(\"SELECT * FROM movies\", engine)\n",
    "\n",
    "# Close connection\n",
    "engine.dispose()\n",
    "\n",
    "print(f\"\\nâœ… Data loaded successfully!\")\n",
    "print(f\"  - Ratings: {len(ratings):,} interactions\")\n",
    "print(f\"  - Movies: {len(movies):,} films\")\n",
    "print(f\"  - Users: {ratings['userId'].nunique():,} unique users\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nðŸ” Ratings dataset:\")\n",
    "print(ratings.head())\n",
    "print(f\"\\nColumns: {ratings.columns.tolist()}\")\n",
    "print(f\"Memory usage: {ratings.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nðŸŽ¬ Movies dataset:\")\n",
    "print(movies.head())\n",
    "print(f\"\\nColumns: {movies.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "234b3361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Filtering columns...\n",
      "âœ… Columns filtered:\n",
      "  - Ratings: ['userId', 'movieId', 'rating', 'timestamp']\n",
      "  - Movies: ['movieId', 'title']\n",
      "\n",
      "ðŸ” Data types check:\n",
      "  Ratings:\n",
      "userId         int64\n",
      "movieId        int64\n",
      "rating       float64\n",
      "timestamp      int64\n",
      "dtype: object\n",
      "\n",
      "  Movies:\n",
      "movieId     int64\n",
      "title      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Filter columns\n",
    "print(\"\\nðŸ”§ Filtering columns...\")\n",
    "\n",
    "# Keep only necessary columns from ratings\n",
    "ratings = ratings[['userId', 'movieId', 'rating', 'timestamp']].copy()\n",
    "\n",
    "# Keep only movieId and title from movies (genres not used in the notebook)\n",
    "movies = movies[['movieId', 'title']].copy()\n",
    "\n",
    "print(f\"âœ… Columns filtered:\")\n",
    "print(f\"  - Ratings: {ratings.columns.tolist()}\")\n",
    "print(f\"  - Movies: {movies.columns.tolist()}\")\n",
    "\n",
    "# Verify data types\n",
    "print(f\"\\nðŸ” Data types check:\")\n",
    "print(f\"  Ratings:\\n{ratings.dtypes}\")\n",
    "print(f\"\\n  Movies:\\n{movies.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48572a8f",
   "metadata": {},
   "source": [
    "### Temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd6d8eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPORAL TRAIN/TEST\n",
      "============================================================\n",
      "\n",
      "â° Sorting ratings by timestamp...\n",
      "\n",
      "ðŸ“Š Split sizes:\n",
      "  - Total: 1,000,000 ratings\n",
      "  - Train (70%): 700,000 ratings\n",
      "  - Test (20%): 200,000 ratings\n",
      "  - Buffer (10%): 100,000 ratings\n",
      "\n",
      "âœ… Train set:\n",
      "  - Size: 700,000\n",
      "  - Date range: 2017-05-13 16:31:36 â†’ 2017-10-15 21:29:23\n",
      "  - Users: 7,250\n",
      "  - Movies: 26,036\n",
      "\n",
      "âœ… Test set:\n",
      "  - Size: 200,000\n",
      "  - Date range: 2017-10-15 21:29:27 â†’ 2017-12-09 20:19:02\n",
      "  - Users: 3,725\n",
      "  - Movies: 14,257\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TEMPORAL TRAIN/TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sort by timestamp\n",
    "print(\"\\nâ° Sorting ratings by timestamp...\")\n",
    "ratings = ratings.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Calculate split indices\n",
    "n_total = len(ratings)\n",
    "n_train = int(n_total * 0.70)\n",
    "n_test = int(n_total * 0.20)\n",
    "# buffer gets the rest (should be ~10%)\n",
    "\n",
    "print(f\"\\nðŸ“Š Split sizes:\")\n",
    "print(f\"  - Total: {n_total:,} ratings\")\n",
    "print(f\"  - Train (70%): {n_train:,} ratings\")\n",
    "print(f\"  - Test (20%): {n_test:,} ratings\")\n",
    "print(f\"  - Buffer (10%): {n_total - n_train - n_test:,} ratings\")\n",
    "\n",
    "# Create splits\n",
    "train_ratings = ratings.iloc[:n_train].copy()\n",
    "test_ratings = ratings.iloc[n_train:n_train+n_test].copy()\n",
    "buffer_ratings = ratings.iloc[n_train+n_test:].copy()\n",
    "\n",
    "# Display info for each split\n",
    "print(f\"\\nâœ… Train set:\")\n",
    "print(f\"  - Size: {len(train_ratings):,}\")\n",
    "print(f\"  - Date range: {pd.to_datetime(train_ratings['timestamp'], unit='s').min()} â†’ {pd.to_datetime(train_ratings['timestamp'], unit='s').max()}\")\n",
    "print(f\"  - Users: {train_ratings['userId'].nunique():,}\")\n",
    "print(f\"  - Movies: {train_ratings['movieId'].nunique():,}\")\n",
    "\n",
    "print(f\"\\nâœ… Test set:\")\n",
    "print(f\"  - Size: {len(test_ratings):,}\")\n",
    "print(f\"  - Date range: {pd.to_datetime(test_ratings['timestamp'], unit='s').min()} â†’ {pd.to_datetime(test_ratings['timestamp'], unit='s').max()}\")\n",
    "print(f\"  - Users: {test_ratings['userId'].nunique():,}\")\n",
    "print(f\"  - Movies: {test_ratings['movieId'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b60a3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df117a4",
   "metadata": {},
   "source": [
    "### Mapping User/Movie ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "542da83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATE USER/MOVIE MAPPINGS\n",
      "============================================================\n",
      "\n",
      "ðŸ”¢ Creating mappings...\n",
      "\n",
      "âœ… Mappings created:\n",
      "  - Users: 7,250 unique users\n",
      "  - Movies: 26,036 unique movies\n",
      "\n",
      "ðŸ”„ Mapping IDs to indices in datasets...\n",
      "\n",
      "ðŸ“Š Missing mappings (users/movies not in train):\n",
      "  - Test: 154,975 users, 3,519 movies\n",
      "  - Buffer: 79,334 users, 2,498 movies\n",
      "\n",
      "âœ… Mappings completed!\n",
      "Train sample with indices:\n",
      "   userId  user_idx  movieId  movie_idx  rating\n",
      "0  110286         0   109388          0     3.5\n",
      "1  112119         1      223          1     4.0\n",
      "2  112119         1    76251          2     4.0\n",
      "3  112119         1    84152          3     4.0\n",
      "4  112119         1    61024          4     3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATE USER/MOVIE MAPPINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create mappings from userId/movieId to continuous indices\n",
    "print(\"\\nðŸ”¢ Creating mappings...\")\n",
    "\n",
    "# Get unique users and movies from TRAIN set only\n",
    "unique_users = train_ratings['userId'].unique()\n",
    "unique_movies = train_ratings['movieId'].unique()\n",
    "\n",
    "# Create bidirectional mappings\n",
    "user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
    "\n",
    "movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(unique_movies)}\n",
    "idx_to_movie = {idx: movie_id for movie_id, idx in movie_to_idx.items()}\n",
    "\n",
    "print(f\"\\nâœ… Mappings created:\")\n",
    "print(f\"  - Users: {len(user_to_idx):,} unique users\")\n",
    "print(f\"  - Movies: {len(movie_to_idx):,} unique movies\")\n",
    "\n",
    "# Add mapped indices to train/test/buffer\n",
    "print(\"\\nðŸ”„ Mapping IDs to indices in datasets...\")\n",
    "\n",
    "train_ratings['user_idx'] = train_ratings['userId'].map(user_to_idx)\n",
    "train_ratings['movie_idx'] = train_ratings['movieId'].map(movie_to_idx)\n",
    "\n",
    "test_ratings['user_idx'] = test_ratings['userId'].map(user_to_idx)\n",
    "test_ratings['movie_idx'] = test_ratings['movieId'].map(movie_to_idx)\n",
    "\n",
    "buffer_ratings['user_idx'] = buffer_ratings['userId'].map(user_to_idx)\n",
    "buffer_ratings['movie_idx'] = buffer_ratings['movieId'].map(movie_to_idx)\n",
    "\n",
    "# Check for missing mappings (users/movies not in train)\n",
    "print(f\"\\nðŸ“Š Missing mappings (users/movies not in train):\")\n",
    "print(f\"  - Test: {test_ratings['user_idx'].isna().sum():,} users, {test_ratings['movie_idx'].isna().sum():,} movies\")\n",
    "print(f\"  - Buffer: {buffer_ratings['user_idx'].isna().sum():,} users, {buffer_ratings['movie_idx'].isna().sum():,} movies\")\n",
    "\n",
    "print(\"\\nâœ… Mappings completed!\")\n",
    "print(f\"Train sample with indices:\")\n",
    "print(train_ratings[['userId', 'user_idx', 'movieId', 'movie_idx', 'rating']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a019bc59",
   "metadata": {},
   "source": [
    "### Cold Start Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b1c8721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COLD START STRATEGY\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Final datasets (with cold start items):\n",
      "  - Train: 700,000 ratings\n",
      "  - Test: 200,000 ratings\n",
      "  - Buffer: 100,000 ratings\n",
      "\n",
      "ðŸŒ¡ï¸ Cold start fallback:\n",
      "  - Global mean rating: 3.540\n",
      "\n",
      "ðŸ“ˆ Cold start breakdown:\n",
      "\n",
      "  Test set:\n",
      "    - Known users: 45,025 (22.5%)\n",
      "    - Known movies: 196,481 (98.2%)\n",
      "    - Both known: 43,145 (21.6%)\n",
      "    - Cold start: 156,855 (78.4%)\n",
      "\n",
      "  Buffer set:\n",
      "    - Known users: 20,666 (20.7%)\n",
      "    - Known movies: 97,502 (97.5%)\n",
      "    - Both known: 18,895 (18.9%)\n",
      "    - Cold start: 81,105 (81.1%)\n",
      "\n",
      "âœ… Strategy: Model predictions for known items, fallback to global mean for cold start\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COLD START STRATEGY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate global statistics for cold start fallback\n",
    "global_mean_rating = train_ratings['rating'].mean()\n",
    "\n",
    "print(f\"\\nðŸ“Š Final datasets (with cold start items):\")\n",
    "print(f\"  - Train: {len(train_ratings):,} ratings\")\n",
    "print(f\"  - Test: {len(test_ratings):,} ratings\")\n",
    "print(f\"  - Buffer: {len(buffer_ratings):,} ratings\")\n",
    "\n",
    "print(f\"\\nðŸŒ¡ï¸ Cold start fallback:\")\n",
    "print(f\"  - Global mean rating: {global_mean_rating:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Cold start breakdown:\")\n",
    "\n",
    "# Test set analysis\n",
    "known_users_test = test_ratings['user_idx'].notna().sum()\n",
    "known_movies_test = test_ratings['movie_idx'].notna().sum()\n",
    "both_known_test = (test_ratings['user_idx'].notna() & test_ratings['movie_idx'].notna()).sum()\n",
    "\n",
    "print(f\"\\n  Test set:\")\n",
    "print(f\"    - Known users: {known_users_test:,} ({known_users_test/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"    - Known movies: {known_movies_test:,} ({known_movies_test/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"    - Both known: {both_known_test:,} ({both_known_test/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"    - Cold start: {len(test_ratings) - both_known_test:,} ({(len(test_ratings) - both_known_test)/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "# Buffer set analysis\n",
    "known_users_buffer = buffer_ratings['user_idx'].notna().sum()\n",
    "known_movies_buffer = buffer_ratings['movie_idx'].notna().sum()\n",
    "both_known_buffer = (buffer_ratings['user_idx'].notna() & buffer_ratings['movie_idx'].notna()).sum()\n",
    "\n",
    "print(f\"\\n  Buffer set:\")\n",
    "print(f\"    - Known users: {known_users_buffer:,} ({known_users_buffer/len(buffer_ratings)*100:.1f}%)\")\n",
    "print(f\"    - Known movies: {known_movies_buffer:,} ({known_movies_buffer/len(buffer_ratings)*100:.1f}%)\")\n",
    "print(f\"    - Both known: {both_known_buffer:,} ({both_known_buffer/len(buffer_ratings)*100:.1f}%)\")\n",
    "print(f\"    - Cold start: {len(buffer_ratings) - both_known_buffer:,} ({(len(buffer_ratings) - both_known_buffer)/len(buffer_ratings)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Strategy: Model predictions for known items, fallback to global mean for cold start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f3b7b",
   "metadata": {},
   "source": [
    "### Sparse Matrix Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e3aad9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPARSE MATRIX CONSTRUCTION\n",
      "============================================================\n",
      "\n",
      "ðŸ”¨ Building sparse matrix from train data...\n",
      "\n",
      "âœ… Sparse matrix created:\n",
      "  - Shape: (7250, 26036) (users x movies)\n",
      "  - Non-zero entries: 700,000\n",
      "  - Sparsity: 99.63%\n",
      "  - Memory size: 5.34 MB\n",
      "\n",
      "ðŸ“Š Matrix statistics:\n",
      "  - Min rating: 0.5\n",
      "  - Max rating: 5.0\n",
      "  - Mean rating: 3.540\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SPARSE MATRIX CONSTRUCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract data for sparse matrix (only train data)\n",
    "print(\"\\nðŸ”¨ Building sparse matrix from train data...\")\n",
    "\n",
    "user_indices = train_ratings['user_idx'].values\n",
    "movie_indices = train_ratings['movie_idx'].values\n",
    "ratings_values = train_ratings['rating'].values\n",
    "\n",
    "# Create sparse matrix\n",
    "n_users = len(user_to_idx)\n",
    "n_movies = len(movie_to_idx)\n",
    "\n",
    "user_item_matrix = csr_matrix(\n",
    "    (ratings_values, (user_indices, movie_indices)),\n",
    "    shape=(n_users, n_movies)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Sparse matrix created:\")\n",
    "print(f\"  - Shape: {user_item_matrix.shape} (users x movies)\")\n",
    "print(f\"  - Non-zero entries: {user_item_matrix.nnz:,}\")\n",
    "print(f\"  - Sparsity: {(1 - user_item_matrix.nnz / (n_users * n_movies)) * 100:.2f}%\")\n",
    "print(f\"  - Memory size: {user_item_matrix.data.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check matrix statistics\n",
    "print(f\"\\nðŸ“Š Matrix statistics:\")\n",
    "print(f\"  - Min rating: {ratings_values.min()}\")\n",
    "print(f\"  - Max rating: {ratings_values.max()}\")\n",
    "print(f\"  - Mean rating: {ratings_values.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928e36f",
   "metadata": {},
   "source": [
    "## SVD Model (Collaborative Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97ce39",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d07ab",
   "metadata": {},
   "source": [
    "#### With n_components set as 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df3ae0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SVD MODEL TRAINING\n",
      "============================================================\n",
      "ðŸ§® Calculating user mean ratings...\n",
      "ðŸ“Š Centering the matrix (subtracting user means)...\n",
      "âœ… Matrix centered (mean: -0.000)\n",
      "\n",
      "\n",
      "ðŸ§  Training SVD model...\n",
      "  - Latent factors: 50\n",
      "  - Matrix shape: (7250, 26036)\n",
      "\n",
      "âœ… Model trained in 1.33 seconds\n",
      "\n",
      "ðŸ“Š Model details:\n",
      "  - User factors shape: (7250, 50)\n",
      "  - Movie factors shape: (26036, 50)\n",
      "  - Explained variance ratio: 0.2466\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SVD MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set number of latent factors\n",
    "n_components = 50\n",
    "\n",
    "# Calculate user means for centering\n",
    "print(\"ðŸ§® Calculating user mean ratings...\")\n",
    "# Calculate mean only on non-zero ratings\n",
    "user_means = np.array([\n",
    "    user_item_matrix.getrow(i).data.mean() if user_item_matrix.getrow(i).nnz > 0 else global_mean_rating\n",
    "    for i in range(user_item_matrix.shape[0])\n",
    "])\n",
    "\n",
    "# Center the matrix\n",
    "print(\"ðŸ“Š Centering the matrix (subtracting user means)...\")\n",
    "user_item_centered = user_item_matrix.copy()\n",
    "for user_idx in range(len(user_means)):\n",
    "    start_idx = user_item_matrix.indptr[user_idx]\n",
    "    end_idx = user_item_matrix.indptr[user_idx + 1]\n",
    "    user_item_centered.data[start_idx:end_idx] -= user_means[user_idx]\n",
    "\n",
    "print(f\"âœ… Matrix centered (mean: {user_item_centered.data.mean():.3f})\\n\")\n",
    "\n",
    "print(f\"\\nðŸ§  Training SVD model...\")\n",
    "print(f\"  - Latent factors: {n_components}\")\n",
    "print(f\"  - Matrix shape: {user_item_centered.shape}\")\n",
    "\n",
    "# Train SVD\n",
    "start_time = time.time()\n",
    "\n",
    "svd_model = TruncatedSVD(\n",
    "    n_components=n_components,\n",
    "    random_state=42,\n",
    "    algorithm='randomized'\n",
    ")\n",
    "\n",
    "# Fit on the user-item matrix\n",
    "user_factors = svd_model.fit_transform(user_item_centered)\n",
    "movie_factors = svd_model.components_.T\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Model trained in {training_time:.2f} seconds\")\n",
    "print(f\"\\nðŸ“Š Model details:\")\n",
    "print(f\"  - User factors shape: {user_factors.shape}\")\n",
    "print(f\"  - Movie factors shape: {movie_factors.shape}\")\n",
    "print(f\"  - Explained variance ratio: {svd_model.explained_variance_ratio_.sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0b859",
   "metadata": {},
   "source": [
    "##### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3e3d1",
   "metadata": {},
   "source": [
    "The explained variance ratio is too low (24%). Let's try other values for n_components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d39ffc",
   "metadata": {},
   "source": [
    "#### Testing Different Values for n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f84ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SVD HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "ðŸ§® Calculating user mean ratings...\n",
      "ðŸ“Š Centering the matrix (subtracting user means)...\n",
      "âœ… Matrix centered (mean: -0.000)\n",
      "\n",
      "\n",
      "ðŸ” Testing different number of latent factors...\n",
      "\n",
      "  n_components= 50 â†’ Variance: 0.2466 (24.66%) | Time: 1.94s\n",
      "  n_components=100 â†’ Variance: 0.3501 (35.01%) | Time: 4.61s\n",
      "  n_components=150 â†’ Variance: 0.4295 (42.95%) | Time: 7.93s\n",
      "  n_components=200 â†’ Variance: 0.4948 (49.48%) | Time: 10.95s\n",
      "  n_components=300 â†’ Variance: 0.5965 (59.65%) | Time: 16.54s\n",
      "  n_components=400 â†’ Variance: 0.6727 (67.27%) | Time: 19.74s\n",
      "  n_components=500 â†’ Variance: 0.7317 (73.17%) | Time: 27.61s\n",
      "  n_components=600 â†’ Variance: 0.7779 (77.79%) | Time: 25.72s\n",
      "  n_components=700 â†’ Variance: 0.8149 (81.49%) | Time: 26.72s\n",
      "  n_components=800 â†’ Variance: 0.8446 (84.46%) | Time: 30.11s\n",
      "\n",
      "ðŸ“Š Results summary:\n",
      "   n_components  explained_variance  training_time\n",
      "0            50            0.246569       1.940508\n",
      "1           100            0.350105       4.607672\n",
      "2           150            0.429523       7.927623\n",
      "3           200            0.494812      10.947075\n",
      "4           300            0.596473      16.537963\n",
      "5           400            0.672683      19.735512\n",
      "6           500            0.731651      27.614994\n",
      "7           600            0.777928      25.724217\n",
      "8           700            0.814857      26.724514\n",
      "9           800            0.844642      30.114500\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SVD HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test different number of components\n",
    "n_components_list = [50, 100, 150, 200, 300, 400, 500, 600, 700, 800]\n",
    "results = []\n",
    "\n",
    "# Calculate user means for centering\n",
    "print(\"ðŸ§® Calculating user mean ratings...\")\n",
    "# Calculate mean only on non-zero ratings\n",
    "user_means = np.array([\n",
    "    user_item_matrix.getrow(i).data.mean() if user_item_matrix.getrow(i).nnz > 0 else global_mean_rating\n",
    "    for i in range(user_item_matrix.shape[0])\n",
    "])\n",
    "\n",
    "# Center the matrix\n",
    "print(\"ðŸ“Š Centering the matrix (subtracting user means)...\")\n",
    "user_item_centered = user_item_matrix.copy()\n",
    "for user_idx in range(len(user_means)):\n",
    "    start_idx = user_item_matrix.indptr[user_idx]\n",
    "    end_idx = user_item_matrix.indptr[user_idx + 1]\n",
    "    user_item_centered.data[start_idx:end_idx] -= user_means[user_idx]\n",
    "\n",
    "print(f\"âœ… Matrix centered (mean: {user_item_centered.data.mean():.3f})\\n\")\n",
    "\n",
    "print(\"\\nðŸ” Testing different number of latent factors...\\n\")\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    svd = TruncatedSVD(\n",
    "        n_components=n_comp,\n",
    "        random_state=42,\n",
    "        algorithm='randomized'\n",
    "    )\n",
    "    \n",
    "    svd.fit(user_item_centered)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    explained_var = svd.explained_variance_ratio_.sum()\n",
    "    \n",
    "    results.append({\n",
    "        'n_components': n_comp,\n",
    "        'explained_variance': explained_var,\n",
    "        'training_time': train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"  n_components={n_comp:3d} â†’ Variance: {explained_var:.4f} ({explained_var*100:.2f}%) | Time: {train_time:.2f}s\")\n",
    "\n",
    "# Display results as DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“Š Results summary:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c169d5a",
   "metadata": {},
   "source": [
    "##### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0f0f7",
   "metadata": {},
   "source": [
    "Based on the hyperparameter tuning above (with properly centered matrix), we observe:\n",
    "\n",
    "- 50 â†’ 100: +10.35% variance (+0.73s)\n",
    "- 100 â†’ 150: +7.94% variance (+1.00s)\n",
    "- 150 â†’ 200: +6.53% variance (+0.66s)\n",
    "- 200 â†’ 300: +10.17% variance (+1.30s)\n",
    "- 300 â†’ 400: +7.62% variance (+3.17s)\n",
    "- **400 â†’ 500: +5.90% variance (+0.52s)** â­\n",
    "- 500 â†’ 600: +4.62% variance (+3.03s)\n",
    "- 600 â†’ 700: +3.70% variance (+1.95s)\n",
    "\n",
    "\n",
    "We select **n_components = 500** for the following reasons:\n",
    "\n",
    "1. **Best benefit/cost ratio in the tested range**\n",
    "   - Strong variance gain (+5.90%) with minimal training time increase (+6.5%)\n",
    "   - Ratio of 11.35% variance per second (best among all high-dimension jumps)\n",
    "   - The 500â†’600 jump shows clear efficiency degradation (+4.62% for +278% time increase)\n",
    "\n",
    "2. **73.17% explained variance**\n",
    "   - Excellent performance for the dataset (1M ratings, 9,189 users, 62,423 movies)\n",
    "   - Significantly surpasses the typical 30-50% range for collaborative filtering systems\n",
    "   - Provides strong signal capture while maintaining fast training time (8.01s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687564f5",
   "metadata": {},
   "source": [
    "#### Training With n_components set as 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9cfdec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL SVD MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "ðŸ§® Calculating user mean ratings...\n",
      "ðŸ“Š Centering the matrix (subtracting user means)...\n",
      "âœ… Matrix centered\n",
      "  - Original mean: 3.540\n",
      "  - Centered mean: -0.000\n",
      "\n",
      "ðŸ§  Training final SVD model...\n",
      "  - Latent factors: 500\n",
      "  - Expected variance: ~52.7%\n",
      "\n",
      "âœ… Model trained in 18.77 seconds\n",
      "\n",
      "ðŸ“Š Final model:\n",
      "  - User factors: (7250, 500)\n",
      "  - Movie factors: (26036, 500)\n",
      "  - Explained variance: 0.7317 (73.17%)\n",
      "  - Memory usage: 126.98 MB\n",
      "\n",
      "âœ… SVD model ready for predictions!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL SVD MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set final hyperparameters\n",
    "n_components = 500\n",
    "\n",
    "# Calculate user means for centering\n",
    "print(\"\\nðŸ§® Calculating user mean ratings...\")\n",
    "# Calculate mean only on non-zero ratings\n",
    "user_means = np.array([\n",
    "    user_item_matrix.getrow(i).data.mean() if user_item_matrix.getrow(i).nnz > 0 else global_mean_rating\n",
    "    for i in range(user_item_matrix.shape[0])\n",
    "])\n",
    "\n",
    "# Center the matrix: subtract user means\n",
    "print(\"ðŸ“Š Centering the matrix (subtracting user means)...\")\n",
    "user_item_centered = user_item_matrix.copy()\n",
    "for user_idx in range(len(user_means)):\n",
    "    # Get non-zero indices for this user\n",
    "    start_idx = user_item_matrix.indptr[user_idx]\n",
    "    end_idx = user_item_matrix.indptr[user_idx + 1]\n",
    "    # Subtract user mean from all ratings\n",
    "    user_item_centered.data[start_idx:end_idx] -= user_means[user_idx]\n",
    "\n",
    "print(f\"âœ… Matrix centered\")\n",
    "print(f\"  - Original mean: {user_item_matrix.data.mean():.3f}\")\n",
    "print(f\"  - Centered mean: {user_item_centered.data.mean():.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ§  Training final SVD model...\")\n",
    "print(f\"  - Latent factors: {n_components}\")\n",
    "print(f\"  - Expected variance: ~52.7%\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "svd_model = TruncatedSVD(\n",
    "    n_components=n_components,\n",
    "    random_state=42,\n",
    "    algorithm='randomized'\n",
    ")\n",
    "\n",
    "user_factors = svd_model.fit_transform(user_item_centered)\n",
    "movie_factors = svd_model.components_.T\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Model trained in {training_time:.2f} seconds\")\n",
    "print(f\"\\nðŸ“Š Final model:\")\n",
    "print(f\"  - User factors: {user_factors.shape}\")\n",
    "print(f\"  - Movie factors: {movie_factors.shape}\")\n",
    "print(f\"  - Explained variance: {svd_model.explained_variance_ratio_.sum():.4f} ({svd_model.explained_variance_ratio_.sum()*100:.2f}%)\")\n",
    "print(f\"  - Memory usage: {(user_factors.nbytes + movie_factors.nbytes) / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nâœ… SVD model ready for predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848742e2",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801e8de",
   "metadata": {},
   "source": [
    "#### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "707a43e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREDICTION FUNCTION\n",
      "============================================================\n",
      "\n",
      "ðŸ§ª Testing prediction function...\n",
      "\n",
      "  User 110286.0, Movie 109388.0\n",
      "  Actual rating: 3.5\n",
      "  Predicted rating: 3.77\n",
      "\n",
      "  Cold start test (unknown user/movie):\n",
      "  Predicted rating: 3.54\n",
      "\n",
      "âœ… Prediction function ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def predict_rating(user_id, movie_id):\n",
    "    \"\"\"\n",
    "    Predict rating for a user-movie pair.\n",
    "    Handles cold start by falling back to global mean.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : int\n",
    "        Original user ID from dataset\n",
    "    movie_id : int\n",
    "        Original movie ID from dataset\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Predicted rating\n",
    "    \"\"\"\n",
    "    # Get indices\n",
    "    user_idx = user_to_idx.get(user_id)\n",
    "    movie_idx = movie_to_idx.get(movie_id)\n",
    "    \n",
    "    # Cold start: user or movie not in training set\n",
    "    if user_idx is None or movie_idx is None:\n",
    "        return global_mean_rating\n",
    "    \n",
    "    # Get factors\n",
    "    user_vec = user_factors[user_idx]\n",
    "    movie_vec = movie_factors[movie_idx]\n",
    "\n",
    "    # Compute dot product (centered prediction)\n",
    "    centered_prediction = np.dot(user_vec, movie_vec)\n",
    "\n",
    "    # Add back user mean to get actual rating\n",
    "    prediction = user_means[user_idx] + centered_prediction\n",
    "\n",
    "    # Clip to valid rating range\n",
    "    prediction = np.clip(prediction, 0.5, 5.0)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test the function\n",
    "print(\"\\nðŸ§ª Testing prediction function...\")\n",
    "test_user = train_ratings.iloc[0]['userId']\n",
    "test_movie = train_ratings.iloc[0]['movieId']\n",
    "test_actual = train_ratings.iloc[0]['rating']\n",
    "\n",
    "predicted = predict_rating(test_user, test_movie)\n",
    "print(f\"\\n  User {test_user}, Movie {test_movie}\")\n",
    "print(f\"  Actual rating: {test_actual}\")\n",
    "print(f\"  Predicted rating: {predicted:.2f}\")\n",
    "\n",
    "# Test cold start\n",
    "print(f\"\\n  Cold start test (unknown user/movie):\")\n",
    "cold_start_pred = predict_rating(999999, 999999)\n",
    "print(f\"  Predicted rating: {cold_start_pred:.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Prediction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dddc3ad",
   "metadata": {},
   "source": [
    "#### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2babdc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Predicting on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [00:23<00:00, 8385.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Regression Metrics:\n",
      "  - RMSE: 1.0722\n",
      "  - MAE: 0.8339\n",
      "\n",
      "ðŸ” Prediction Analysis:\n",
      "  - Mean actual rating: 3.546\n",
      "  - Mean predicted rating: 3.542\n",
      "  - Std actual: 1.090\n",
      "  - Std predicted: 0.274\n",
      "\n",
      "â„ï¸ Cold Start Breakdown:\n",
      "  - Known pairs: 43,145 (21.6%)\n",
      "  - Cold start pairs: 156,855 (78.4%)\n",
      "\n",
      "âœ… Evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“Š Predicting on test set...\")\n",
    "\n",
    "# Predict all test ratings\n",
    "test_predictions = []\n",
    "test_actuals = []\n",
    "\n",
    "for idx, row in tqdm(test_ratings.iterrows(), total=len(test_ratings), desc=\"Predicting\"):\n",
    "    pred = predict_rating(row['userId'], row['movieId'])\n",
    "    test_predictions.append(pred)\n",
    "    test_actuals.append(row['rating'])\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_actuals = np.array(test_actuals)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test_actuals, test_predictions))\n",
    "mae = mean_absolute_error(test_actuals, test_predictions)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Regression Metrics:\")\n",
    "print(f\"  - RMSE: {rmse:.4f}\")\n",
    "print(f\"  - MAE: {mae:.4f}\")\n",
    "\n",
    "# Analyze predictions\n",
    "print(f\"\\nðŸ” Prediction Analysis:\")\n",
    "print(f\"  - Mean actual rating: {test_actuals.mean():.3f}\")\n",
    "print(f\"  - Mean predicted rating: {test_predictions.mean():.3f}\")\n",
    "print(f\"  - Std actual: {test_actuals.std():.3f}\")\n",
    "print(f\"  - Std predicted: {test_predictions.std():.3f}\")\n",
    "\n",
    "# Cold start breakdown\n",
    "cold_start_mask = (test_ratings['user_idx'].isna() | test_ratings['movie_idx'].isna())\n",
    "n_cold_start = cold_start_mask.sum()\n",
    "n_known = len(test_ratings) - n_cold_start\n",
    "\n",
    "print(f\"\\nâ„ï¸ Cold Start Breakdown:\")\n",
    "print(f\"  - Known pairs: {n_known:,} ({n_known/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"  - Cold start pairs: {n_cold_start:,} ({n_cold_start/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af125846",
   "metadata": {},
   "source": [
    "#### Evaluate on Known Pairs Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f562bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION ON KNOWN PAIRS ONLY\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Evaluating on 43,145 known pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43145/43145 [00:04<00:00, 9086.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Metrics on KNOWN pairs only:\n",
      "  - RMSE: 0.9444\n",
      "  - MAE: 0.6985\n",
      "\n",
      "ðŸ” Prediction Analysis (known pairs):\n",
      "  - Mean actual: 3.414\n",
      "  - Mean predicted: 3.548\n",
      "  - Std actual: 1.026\n",
      "  - Std predicted: 0.590\n",
      "\n",
      "ðŸ“Š Prediction distribution:\n",
      "  - Min: 0.50\n",
      "  - 25%: 3.16\n",
      "  - 50%: 3.56\n",
      "  - 75%: 3.98\n",
      "  - Max: 5.00\n",
      "\n",
      "âœ… Known pairs evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION ON KNOWN PAIRS ONLY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter only known pairs (both user and movie in training)\n",
    "known_mask = test_ratings['user_idx'].notna() & test_ratings['movie_idx'].notna()\n",
    "test_ratings_known = test_ratings[known_mask].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Evaluating on {len(test_ratings_known):,} known pairs...\")\n",
    "\n",
    "# Predict only on known pairs\n",
    "known_predictions = []\n",
    "known_actuals = []\n",
    "\n",
    "for idx, row in tqdm(test_ratings_known.iterrows(), total=len(test_ratings_known), desc=\"Predicting\"):\n",
    "    pred = predict_rating(row['userId'], row['movieId'])\n",
    "    known_predictions.append(pred)\n",
    "    known_actuals.append(row['rating'])\n",
    "\n",
    "known_predictions = np.array(known_predictions)\n",
    "known_actuals = np.array(known_actuals)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_known = np.sqrt(mean_squared_error(known_actuals, known_predictions))\n",
    "mae_known = mean_absolute_error(known_actuals, known_predictions)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Metrics on KNOWN pairs only:\")\n",
    "print(f\"  - RMSE: {rmse_known:.4f}\")\n",
    "print(f\"  - MAE: {mae_known:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Prediction Analysis (known pairs):\")\n",
    "print(f\"  - Mean actual: {known_actuals.mean():.3f}\")\n",
    "print(f\"  - Mean predicted: {known_predictions.mean():.3f}\")\n",
    "print(f\"  - Std actual: {known_actuals.std():.3f}\")\n",
    "print(f\"  - Std predicted: {known_predictions.std():.3f}\")\n",
    "\n",
    "# Show distribution of predictions\n",
    "print(f\"\\nðŸ“Š Prediction distribution:\")\n",
    "print(f\"  - Min: {known_predictions.min():.2f}\")\n",
    "print(f\"  - 25%: {np.percentile(known_predictions, 25):.2f}\")\n",
    "print(f\"  - 50%: {np.percentile(known_predictions, 50):.2f}\")\n",
    "print(f\"  - 75%: {np.percentile(known_predictions, 75):.2f}\")\n",
    "print(f\"  - Max: {known_predictions.max():.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Known pairs evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279a531",
   "metadata": {},
   "source": [
    "## Hybrid Model (Collaborative Filtering + Content-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682251a7",
   "metadata": {},
   "source": [
    "### Load Genome Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cefcc377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOAD GENOME FEATURES\n",
      "============================================================\n",
      "\n",
      "ðŸ“¥ Loading genome data...\n",
      "\n",
      "âœ… Genome data loaded:\n",
      "  - Genome scores: 15,584,448 rows\n",
      "  - Genome tags: 1,128 tags\n",
      "\n",
      "ðŸ” Genome scores sample:\n",
      "   movieId  tagId  relevance\n",
      "0        1      1    0.02875\n",
      "1        1      2    0.02375\n",
      "2        1      3    0.06250\n",
      "3        1      4    0.07575\n",
      "4        1      5    0.14075\n",
      "\n",
      "Columns: ['movieId', 'tagId', 'relevance']\n",
      "\n",
      "ðŸŽ¬ Genome tags sample:\n",
      "   tagId           tag\n",
      "0      1           007\n",
      "1      2  007 (series)\n",
      "2      3  18th century\n",
      "3      4         1920s\n",
      "4      5         1930s\n",
      "\n",
      "Columns: ['tagId', 'tag']\n",
      "\n",
      "ðŸ“Š Coverage:\n",
      "  - Movies in genome: 13,816\n",
      "  - Movies in train: 26,036\n",
      "  - Overlap: 12,221 (46.9%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOAD GENOME FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load genome data\n",
    "print(\"\\nðŸ“¥ Loading genome data...\")\n",
    "genome_scores = pd.read_csv(\"raw/genome-scores.csv\")\n",
    "genome_tags = pd.read_csv(\"raw/genome-tags.csv\")\n",
    "\n",
    "print(f\"\\nâœ… Genome data loaded:\")\n",
    "print(f\"  - Genome scores: {len(genome_scores):,} rows\")\n",
    "print(f\"  - Genome tags: {len(genome_tags):,} tags\")\n",
    "\n",
    "# Check structure\n",
    "print(f\"\\nðŸ” Genome scores sample:\")\n",
    "print(genome_scores.head())\n",
    "print(f\"\\nColumns: {genome_scores.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¬ Genome tags sample:\")\n",
    "print(genome_tags.head())\n",
    "print(f\"\\nColumns: {genome_tags.columns.tolist()}\")\n",
    "\n",
    "# Check coverage\n",
    "genome_movie_ids = genome_scores['movieId'].unique()\n",
    "train_movie_ids = train_ratings['movieId'].unique()\n",
    "overlap = len(set(genome_movie_ids) & set(train_movie_ids))\n",
    "\n",
    "print(f\"\\nðŸ“Š Coverage:\")\n",
    "print(f\"  - Movies in genome: {len(genome_movie_ids):,}\")\n",
    "print(f\"  - Movies in train: {len(train_movie_ids):,}\")\n",
    "print(f\"  - Overlap: {overlap:,} ({overlap/len(train_movie_ids)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221cd45",
   "metadata": {},
   "source": [
    "### Create Movie Embedding with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "428754fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATE MOVIE EMBEDDINGS WITH PCA\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Reshaping genome scores to wide format...\n",
      "âœ… Wide format created: (13816, 1128)\n",
      "  - Movies: 13,816\n",
      "  - Tags: 1,128\n",
      "\n",
      "ðŸ“Š Standardizing features...\n",
      "\n",
      "ðŸ§  Testing different PCA dimensions...\n",
      "Original: 1128 genome tags\n",
      "\n",
      "   32 dims â†’ Variance: 0.5241 (52.41%) | Info loss: 47.59%\n",
      "   64 dims â†’ Variance: 0.6172 (61.72%) | Info loss: 38.28%\n",
      "  128 dims â†’ Variance: 0.7092 (70.92%) | Info loss: 29.08%\n",
      "  256 dims â†’ Variance: 0.8084 (80.84%) | Info loss: 19.16%\n",
      "\n",
      "ðŸ“Š PCA Dimensions Comparison:\n",
      "   n_components  explained_variance  info_loss\n",
      "0            32            0.524084   0.475916\n",
      "1            64            0.617236   0.382764\n",
      "2           128            0.709170   0.290830\n",
      "3           256            0.808443   0.191557\n",
      "\n",
      "ðŸŽ¯ Selected: 128 dimensions\n",
      "   Reducing 1128 â†’ 128 dims...\n",
      "\n",
      "âœ… PCA complete:\n",
      "  - Shape: (13816, 128)\n",
      "  - Explained variance: 0.7092 (70.92%)\n",
      "\n",
      "ðŸŽ¬ Movie embeddings created:\n",
      "   movieId    g_emb_0    g_emb_1    g_emb_2    g_emb_3    g_emb_4    g_emb_5  \\\n",
      "0        1   5.580747  18.611650  16.136008 -10.331581  11.373028  12.751481   \n",
      "1        2 -11.108714  13.989456   7.307711  -9.619813   2.111028   5.069491   \n",
      "2        3  -9.607064  -1.298562   4.212266   2.363856   1.961193   1.262234   \n",
      "3        4  -8.162947  -6.346217   5.327229   0.797487   0.423006  -2.181344   \n",
      "4        5 -10.634440  -2.242585   8.047873   0.584368   3.230923  -1.263202   \n",
      "\n",
      "    g_emb_6   g_emb_7   g_emb_8   g_emb_9   g_emb_10   g_emb_11  g_emb_12  \\\n",
      "0  3.018454 -6.520241  8.494786 -6.138669  10.335674  11.509018  6.309237   \n",
      "1 -0.301300 -7.396525  4.811967 -0.296635   2.788079   4.641264 -1.938063   \n",
      "2  1.321189  0.344821  0.539595 -1.326429  -1.535054  -1.480891  2.911853   \n",
      "3  3.510429 -1.652275  0.060991  2.596175  -3.639330   2.320975  0.060236   \n",
      "4  4.077310 -2.192711  0.611377 -0.085849  -2.793356   2.651755  4.130027   \n",
      "\n",
      "   g_emb_13  g_emb_14  g_emb_15  g_emb_16  g_emb_17  g_emb_18  g_emb_19  \\\n",
      "0  0.571060  1.954577  6.442428  4.054959  3.936362 -5.467274 -0.158597   \n",
      "1  1.780079  2.902605 -2.311234  0.392107  2.145626  5.036400 -0.851496   \n",
      "2 -1.678373 -0.425301  1.360023 -0.502868 -1.158575 -0.009231 -2.909714   \n",
      "3  0.279206 -0.101570  3.103973 -2.757577  1.125340 -0.973871  0.276852   \n",
      "4 -2.254360 -3.649411  0.993299 -1.256237 -0.388498 -0.947345 -2.927226   \n",
      "\n",
      "   g_emb_20  g_emb_21  g_emb_22  g_emb_23  g_emb_24  g_emb_25  g_emb_26  \\\n",
      "0 -3.639806 -2.246724  0.759003 -2.500737  4.849473 -6.645807  3.861985   \n",
      "1  0.315448  2.985293 -0.949763 -3.923803  4.258579  0.758065  6.526670   \n",
      "2 -2.416258 -1.587242 -3.004408  0.644736  3.475851 -1.210752 -1.794624   \n",
      "3 -1.830730  2.210295 -2.545583  0.282986  0.425691  0.206914 -1.163326   \n",
      "4 -1.018543  0.217061 -2.530093  1.957372  4.651359  3.143142 -1.698073   \n",
      "\n",
      "   g_emb_27  g_emb_28  g_emb_29  g_emb_30  g_emb_31  g_emb_32  g_emb_33  \\\n",
      "0  2.155995  1.448502 -1.983097 -0.256803  2.297844  1.654793 -2.959904   \n",
      "1  4.639108 -6.670843 -0.350166 -4.264720 -0.310740 -1.018323 -0.428883   \n",
      "2  0.091656  2.312029 -2.303533  0.002346 -2.640011 -2.840635  0.545262   \n",
      "3 -1.553388  0.725715  1.331040  1.765666  0.289232  0.630903  0.677716   \n",
      "4 -1.104639  2.420600 -3.439260 -1.625804 -1.869539 -6.039657  2.347626   \n",
      "\n",
      "   g_emb_34  g_emb_35  g_emb_36  g_emb_37  g_emb_38  g_emb_39  g_emb_40  \\\n",
      "0 -4.841027  1.859411  0.093547  2.881813  5.060927 -0.382158  2.482271   \n",
      "1 -0.852014 -0.940546 -4.295157  1.334010  0.280379 -1.026878  1.504669   \n",
      "2  0.671092 -2.130392  3.954791 -2.596515  0.042595  1.958980 -1.347441   \n",
      "3  1.433956 -0.462690  0.332059  0.325469  0.381592  0.466444  0.209045   \n",
      "4  0.413389 -2.848308  4.889093 -4.624829 -0.191802  1.118864 -0.000004   \n",
      "\n",
      "   g_emb_41  g_emb_42  g_emb_43  g_emb_44  g_emb_45  g_emb_46  g_emb_47  \\\n",
      "0 -3.630686  2.832731  0.021427  3.615390  3.896668 -1.637939 -0.555389   \n",
      "1  3.586977 -8.199558  3.668983 -0.687391  2.959579  1.173506 -1.733455   \n",
      "2  1.158734 -0.299796  0.240334 -0.533947  0.178091 -3.293873  1.911326   \n",
      "3 -0.358435 -0.034616 -0.174034  0.589390  0.263222 -0.079843  0.404144   \n",
      "4  1.327125  1.213602  1.485835 -0.418055 -0.076937 -1.850366  1.324994   \n",
      "\n",
      "   g_emb_48  g_emb_49  g_emb_50  g_emb_51  g_emb_52  g_emb_53  g_emb_54  \\\n",
      "0 -2.630802  1.031356 -1.201671 -3.525363  0.894204 -2.128789  2.039309   \n",
      "1 -2.854031 -0.608144  2.334900  0.005545 -2.654148  0.471448 -3.156877   \n",
      "2 -2.193500 -0.913820  0.704039 -0.684954 -0.632258  2.466309  1.758531   \n",
      "3  0.717012 -0.643296 -1.819392 -0.262590 -1.184850 -0.368629 -0.529475   \n",
      "4 -1.190626 -1.109407 -0.275379 -0.565284 -0.582997  1.659699  1.306465   \n",
      "\n",
      "   g_emb_55  g_emb_56  g_emb_57  g_emb_58  g_emb_59  g_emb_60  g_emb_61  \\\n",
      "0  2.412945  2.852311  2.766569  0.917861  2.515120 -5.980960 -4.228193   \n",
      "1  0.401709 -0.715644 -1.413151 -2.228006 -0.744708  2.027516  0.546443   \n",
      "2 -0.804066 -0.800903  1.305440  0.098741  0.066194 -2.903977  0.312294   \n",
      "3 -0.142562 -0.910525 -1.351818  1.653936  0.375286 -1.424858  0.050689   \n",
      "4 -2.240365 -1.126801  1.047139  1.262937 -0.283779 -1.620765  1.143135   \n",
      "\n",
      "   g_emb_62  g_emb_63  g_emb_64  g_emb_65  g_emb_66  g_emb_67  g_emb_68  \\\n",
      "0  0.570172 -0.211148  3.028797  0.360491  3.256129 -1.579229  3.740984   \n",
      "1 -0.758911 -0.922753  0.642945  2.919825  1.126989  2.186678  0.996129   \n",
      "2 -0.858162 -0.622226  1.426648 -0.640239 -1.403632  1.072193 -0.734465   \n",
      "3 -0.662025 -0.613431  1.655973  1.106537 -0.007269 -0.186522  2.528711   \n",
      "4 -2.478054 -1.475188  0.454561 -1.343003  0.829842  1.894999  1.704249   \n",
      "\n",
      "   g_emb_69  g_emb_70  g_emb_71  g_emb_72  g_emb_73  g_emb_74  g_emb_75  \\\n",
      "0 -1.616500 -0.149552  3.298642  4.337503 -2.412403 -3.081225  1.065975   \n",
      "1 -1.494153  1.570543 -0.481797 -2.237680  0.732044 -0.632103  1.074750   \n",
      "2  1.286964  0.732477 -0.598574 -0.457514 -0.187915  0.169459  0.989455   \n",
      "3  0.549625  1.260740 -2.046728  0.217796  0.806479  0.378172  1.207379   \n",
      "4  0.246718  1.883249 -2.771169 -1.718005 -3.576918 -1.708423  3.155260   \n",
      "\n",
      "   g_emb_76  g_emb_77  g_emb_78  g_emb_79  g_emb_80  g_emb_81  g_emb_82  \\\n",
      "0  3.312619  1.290583 -0.227615 -1.241523 -1.436474 -0.930638  0.232003   \n",
      "1 -1.577001 -2.026343  0.207426 -5.289385 -0.801176  0.381556  1.380806   \n",
      "2  0.301773 -2.276667  0.009536 -0.176696 -1.282647  0.350303 -0.117283   \n",
      "3 -0.722283 -0.221292  0.020140 -0.243269 -0.360804 -0.312307  0.501720   \n",
      "4 -0.062051 -0.681820 -0.267977  0.549263 -1.221566 -1.625999  0.424060   \n",
      "\n",
      "   g_emb_83  g_emb_84  g_emb_85  g_emb_86  g_emb_87  g_emb_88  g_emb_89  \\\n",
      "0  0.731357  2.139310  0.168452 -2.545205  1.047411  1.219002 -1.763560   \n",
      "1  2.291770  0.372963  0.621407 -0.394821 -3.090693 -0.581615  1.121376   \n",
      "2  0.258227 -0.372285  0.169483 -0.806736  0.190567 -0.120043  0.843914   \n",
      "3  0.298443 -0.725156  0.431269  0.679304 -0.701210 -0.564479 -0.294700   \n",
      "4  0.589019  0.879104  0.282576  0.672930 -1.431457 -0.909447 -1.087957   \n",
      "\n",
      "   g_emb_90  g_emb_91  g_emb_92  g_emb_93  g_emb_94  g_emb_95  g_emb_96  \\\n",
      "0  0.526439 -0.777652 -1.284233 -1.615931 -0.444038 -0.064142 -1.711458   \n",
      "1  1.987808  0.149372  0.563805  1.909611 -0.489567 -1.401201  1.791789   \n",
      "2 -0.572126 -0.051075 -1.260868  1.134244  0.521544 -0.176624  1.467824   \n",
      "3  0.782141 -1.151629 -0.312007 -0.883179  0.131850  0.193162 -0.113972   \n",
      "4 -2.309208  0.681555 -0.447562  1.994106 -0.265600  0.787625 -0.799564   \n",
      "\n",
      "   g_emb_97  g_emb_98  g_emb_99  g_emb_100  g_emb_101  g_emb_102  g_emb_103  \\\n",
      "0 -0.681995 -0.618723 -0.220289   1.257493  -0.629287   0.571004   1.121686   \n",
      "1  3.081990  1.099992 -1.014435  -2.744149   1.591859   1.474647   0.360859   \n",
      "2  0.496768 -1.192634  1.166658  -0.847291   2.976007  -0.610986   1.160494   \n",
      "3 -0.215696 -0.784305 -0.342884  -0.129449   0.816859   0.441067  -0.220228   \n",
      "4  1.085262 -1.785767 -1.487282  -0.285071   0.739319   1.603477  -0.643558   \n",
      "\n",
      "   g_emb_104  g_emb_105  g_emb_106  g_emb_107  g_emb_108  g_emb_109  \\\n",
      "0   0.406886  -0.573677   2.661172  -3.137591   1.684373  -0.504880   \n",
      "1   0.707259  -0.336688  -4.252149  -1.283032   1.193395  -1.003046   \n",
      "2  -0.425644   0.005416   0.656158  -0.292011  -0.191584  -0.480066   \n",
      "3  -0.143508   0.799089  -1.526662  -0.544368   0.427707  -0.126181   \n",
      "4  -1.111488   0.881885  -0.819330  -0.309611   0.952799  -0.015276   \n",
      "\n",
      "   g_emb_110  g_emb_111  g_emb_112  g_emb_113  g_emb_114  g_emb_115  \\\n",
      "0   2.052063  -0.469736   3.864811   1.343543  -0.823025  -1.491997   \n",
      "1   0.001617  -0.939575   1.732901   1.054389  -0.109444  -0.770616   \n",
      "2  -0.701725   0.651123  -0.058732  -0.120546   1.621112   0.032872   \n",
      "3  -1.010386   0.618205   0.496126   0.470100   0.467406  -0.409944   \n",
      "4  -0.230419  -2.532998  -0.404364  -0.673893   2.116346  -0.261747   \n",
      "\n",
      "   g_emb_116  g_emb_117  g_emb_118  g_emb_119  g_emb_120  g_emb_121  \\\n",
      "0   0.603579  -1.435218  -0.998953   1.884366  -0.760462  -0.332193   \n",
      "1  -0.344701  -3.369909  -1.671539  -1.944208  -0.036832   1.945114   \n",
      "2   1.045329  -1.105559   1.077427  -0.427829  -0.490836  -0.910775   \n",
      "3   0.735966  -0.249213   0.028537  -0.335624  -1.258734   1.382618   \n",
      "4  -1.782730  -0.650296   0.064012   0.475610  -0.987277   1.601810   \n",
      "\n",
      "   g_emb_122  g_emb_123  g_emb_124  g_emb_125  g_emb_126  g_emb_127  \n",
      "0   2.837811  -1.045898   2.109190  -1.259248  -0.822675   3.385259  \n",
      "1   0.654553  -0.260888   3.074599  -2.150319   0.283933   1.025755  \n",
      "2   0.102689  -1.679347   0.483812  -0.964531   0.994120  -1.796231  \n",
      "3   0.091780  -0.500546  -1.739612  -0.255560   0.450317  -0.106489  \n",
      "4   0.458037   0.660621   1.204767  -0.379153   1.299459   0.020433  \n",
      "\n",
      "Columns: ['movieId', 'g_emb_0', 'g_emb_1', 'g_emb_2', 'g_emb_3']... (total: 129)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATE MOVIE EMBEDDINGS WITH PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reshape genome scores to wide format (movies Ã— tags)\n",
    "print(\"\\nðŸ”„ Reshaping genome scores to wide format...\")\n",
    "genome_wide = genome_scores.pivot(index='movieId', columns='tagId', values='relevance')\n",
    "\n",
    "print(f\"âœ… Wide format created: {genome_wide.shape}\")\n",
    "print(f\"  - Movies: {genome_wide.shape[0]:,}\")\n",
    "print(f\"  - Tags: {genome_wide.shape[1]:,}\")\n",
    "\n",
    "# Fill missing values with 0 (movies not scored on some tags)\n",
    "genome_wide = genome_wide.fillna(0)\n",
    "\n",
    "# Standardize features (center + scale)\n",
    "print(\"\\nðŸ“Š Standardizing features...\")\n",
    "scaler = StandardScaler()\n",
    "genome_scaled = scaler.fit_transform(genome_wide)\n",
    "\n",
    "# Test different PCA dimensions\n",
    "print(f\"\\nðŸ§  Testing different PCA dimensions...\")\n",
    "print(f\"Original: {genome_wide.shape[1]} genome tags\\n\")\n",
    "\n",
    "pca_dims = [32, 64, 128, 256]\n",
    "pca_results = []\n",
    "\n",
    "for n_comp in pca_dims:\n",
    "    # Apply PCA\n",
    "    pca_test = PCA(n_components=n_comp, random_state=42)\n",
    "    embeddings_test = pca_test.fit_transform(genome_scaled)\n",
    "    \n",
    "    explained_var = pca_test.explained_variance_ratio_.sum()\n",
    "    \n",
    "    pca_results.append({\n",
    "        'n_components': n_comp,\n",
    "        'explained_variance': explained_var,\n",
    "        'info_loss': 1 - explained_var\n",
    "    })\n",
    "    \n",
    "    print(f\"  {n_comp:3d} dims â†’ Variance: {explained_var:.4f} ({explained_var*100:.2f}%) | Info loss: {(1-explained_var)*100:.2f}%\")\n",
    "\n",
    "# Display results summary\n",
    "results_df = pd.DataFrame(pca_results)\n",
    "print(\"\\nðŸ“Š PCA Dimensions Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Choose optimal dimension\n",
    "n_components_pca = 128\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Selected: {n_components_pca} dimensions\")\n",
    "print(f\"   Reducing {genome_wide.shape[1]} â†’ {n_components_pca} dims...\")\n",
    "\n",
    "# Final PCA with chosen dimension\n",
    "pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "genome_embeddings = pca.fit_transform(genome_scaled)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "print(f\"\\nâœ… PCA complete:\")\n",
    "print(f\"  - Shape: {genome_embeddings.shape}\")\n",
    "print(f\"  - Explained variance: {explained_var:.4f} ({explained_var*100:.2f}%)\")\n",
    "\n",
    "# Create DataFrame\n",
    "movie_embeddings_df = pd.DataFrame(\n",
    "    genome_embeddings,\n",
    "    index=genome_wide.index,\n",
    "    columns=[f'g_emb_{i}' for i in range(n_components_pca)]\n",
    ")\n",
    "movie_embeddings_df = movie_embeddings_df.reset_index()\n",
    "\n",
    "print(f\"\\nðŸŽ¬ Movie embeddings created:\")\n",
    "print(movie_embeddings_df.head())\n",
    "print(f\"\\nColumns: {list(movie_embeddings_df.columns[:5])}... (total: {len(movie_embeddings_df.columns)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c5594",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3a698",
   "metadata": {},
   "source": [
    "We select **128 dimensions** for genome tag embeddings:\n",
    "\n",
    "1. **Strong variance retention: 70.92%**\n",
    "   - Retains over 70% of the original 1,128 genome tags information\n",
    "   - Loses only 29% vs 38% with 64 dims\n",
    "\n",
    "2. **Optimal gain/complexity trade-off**\n",
    "   - Marginal gain remains strong (+9.20%)\n",
    "   - Efficiency per dimension (0.14%) is still reasonable\n",
    "   - Beyond 128, efficiency drops to 0.08% per dimension\n",
    "\n",
    "The choice of 128 provides the best balance between information retention and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f79f66",
   "metadata": {},
   "source": [
    "### Build Hybrid Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "071d44fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILD HYBRID TRAINING DATASET\n",
      "============================================================\n",
      "\n",
      "ðŸ”— Merging train ratings with movie embeddings...\n",
      "\n",
      "ðŸ“Š Coverage:\n",
      "  - Original train: 700,000 ratings\n",
      "  - With genome: 673,543 ratings (96.2%)\n",
      "\n",
      "ðŸ”® Generating SVD predictions by batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVD predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:04<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ—ï¸ Building feature matrix by batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:05<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Hybrid dataset created:\n",
      "  - Shape: (673543, 629)\n",
      "  - Features breakdown:\n",
      "    â€¢ SVD prediction: 1\n",
      "    â€¢ User factors: 600\n",
      "    â€¢ Movie embeddings: 128\n",
      "    â€¢ Total: 629\n",
      "  - Target: 673,543 ratings\n",
      "  - Memory: 3232.26 MB\n",
      "\n",
      "âœ… Hybrid training dataset ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BUILD HYBRID TRAINING DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter train ratings to only movies with genome embeddings\n",
    "print(\"\\nðŸ”— Merging train ratings with movie embeddings...\")\n",
    "train_with_genome = train_ratings.merge(\n",
    "    movie_embeddings_df, \n",
    "    on='movieId', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Coverage:\")\n",
    "print(f\"  - Original train: {len(train_ratings):,} ratings\")\n",
    "print(f\"  - With genome: {len(train_with_genome):,} ratings ({len(train_with_genome)/len(train_ratings)*100:.1f}%)\")\n",
    "\n",
    "# Generate SVD predictions by batch\n",
    "print(\"\\nðŸ”® Generating SVD predictions by batch...\")\n",
    "batch_size = 100000\n",
    "n_samples = len(train_with_genome)\n",
    "n_batches = (n_samples // batch_size) + 1\n",
    "\n",
    "svd_predictions = []\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"SVD predictions\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, n_samples)\n",
    "    \n",
    "    batch = train_with_genome.iloc[start_idx:end_idx]\n",
    "    user_indices = batch['user_idx'].values.astype(int)\n",
    "    movie_indices = batch['movie_idx'].values.astype(int)\n",
    "    \n",
    "    # Vectorized computation for this batch\n",
    "    centered_pred = np.sum(\n",
    "        user_factors[user_indices] * movie_factors[movie_indices], \n",
    "        axis=1\n",
    "    )\n",
    "    batch_preds = user_means[user_indices] + centered_pred\n",
    "    svd_predictions.extend(batch_preds)\n",
    "\n",
    "train_with_genome['svd_prediction'] = svd_predictions\n",
    "\n",
    "# Build feature matrix by batch\n",
    "print(\"\\nðŸ—ï¸ Building feature matrix by batch...\")\n",
    "genome_cols = [f'g_emb_{i}' for i in range(128)]\n",
    "\n",
    "X_train_hybrid = []\n",
    "y_train_hybrid = []\n",
    "\n",
    "for i in tqdm(range(n_batches), desc=\"Building features\"):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, n_samples)\n",
    "    \n",
    "    batch = train_with_genome.iloc[start_idx:end_idx]\n",
    "    user_indices = batch['user_idx'].values.astype(int)\n",
    "    \n",
    "    # Build batch features\n",
    "    X_svd = batch['svd_prediction'].values.reshape(-1, 1)\n",
    "    X_user = user_factors[user_indices]\n",
    "    X_movie = batch[genome_cols].values\n",
    "    \n",
    "    X_batch = np.hstack([X_svd, X_user, X_movie])\n",
    "    X_train_hybrid.append(X_batch)\n",
    "    y_train_hybrid.extend(batch['rating'].values)\n",
    "\n",
    "# Concatenate all batches\n",
    "X_train_hybrid = np.vstack(X_train_hybrid)\n",
    "y_train_hybrid = np.array(y_train_hybrid)\n",
    "\n",
    "print(f\"\\nâœ… Hybrid dataset created:\")\n",
    "print(f\"  - Shape: {X_train_hybrid.shape}\")\n",
    "print(f\"  - Features breakdown:\")\n",
    "print(f\"    â€¢ SVD prediction: 1\")\n",
    "print(f\"    â€¢ User factors: 600\")\n",
    "print(f\"    â€¢ Movie embeddings: 128\")\n",
    "print(f\"    â€¢ Total: {X_train_hybrid.shape[1]}\")\n",
    "print(f\"  - Target: {len(y_train_hybrid):,} ratings\")\n",
    "print(f\"  - Memory: {X_train_hybrid.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nâœ… Hybrid training dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6e426c",
   "metadata": {},
   "source": [
    "### Train Hybrid Model (SVD + Factors + Movie Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c1d363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN HYBRID MODEL\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Dataset split:\n",
      "  - Train: 572,512 samples\n",
      "  - Validation: 101,031 samples\n",
      "\n",
      "ðŸ§  Training with mini-batch online learning...\n",
      "\n",
      "ðŸ“ Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:04<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”® Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Validation Metrics:\n",
      "  - RMSE: 2.5200\n",
      "  - MAE: 2.1443\n",
      "  - Training time: 0.23 minutes\n",
      "\n",
      "ðŸ” Prediction Analysis:\n",
      "  - Mean actual: 3.533\n",
      "  - Mean predicted: 3.023\n",
      "  - Std actual: 1.073\n",
      "  - Std predicted: 2.233\n",
      "\n",
      "ðŸŽ¯ Comparison with SVD baseline:\n",
      "  - SVD (known pairs): RMSE 0.9444\n",
      "  - Hybrid model: RMSE 2.5200\n",
      "  âš ï¸ Regression: 166.84%\n",
      "\n",
      "âœ… Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAIN HYBRID MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create validation set\n",
    "val_size = int(len(X_train_hybrid) * 0.15)\n",
    "train_size = len(X_train_hybrid) - val_size\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset split:\")\n",
    "print(f\"  - Train: {train_size:,} samples\")\n",
    "print(f\"  - Validation: {val_size:,} samples\")\n",
    "\n",
    "X_train_full = X_train_hybrid[:train_size]\n",
    "y_train_full = y_train_hybrid[:train_size]\n",
    "X_val = X_train_hybrid[train_size:]\n",
    "y_val = y_train_hybrid[train_size:]\n",
    "\n",
    "batch_size = 100000\n",
    "n_batches = (train_size // batch_size) + 1\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nðŸ§  Training with mini-batch online learning...\")\n",
    "hybrid_model = SGDRegressor(\n",
    "    learning_rate='constant',\n",
    "    eta0=0.001,\n",
    "    penalty='l2',\n",
    "    alpha=0.0001,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train by batches\n",
    "n_epochs = 3\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nðŸ“ Epoch {epoch + 1}/{n_epochs}\")\n",
    "    indices = np.random.permutation(train_size)\n",
    "    \n",
    "    for i in tqdm(range(n_batches), desc=f\"  Batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, train_size)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        X_batch = X_train_full[batch_indices]\n",
    "        y_batch = y_train_full[batch_indices]\n",
    "        X_batch_scaled = X_batch\n",
    "        \n",
    "        hybrid_model.partial_fit(X_batch_scaled, y_batch)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nðŸ”® Evaluating on validation set...\")\n",
    "val_predictions = []\n",
    "val_batch_size = 100000\n",
    "n_val_batches = (val_size // val_batch_size) + 1\n",
    "\n",
    "for i in tqdm(range(n_val_batches), desc=\"Validation\"):\n",
    "    start_idx = i * val_batch_size\n",
    "    end_idx = min((i + 1) * val_batch_size, val_size)\n",
    "    X_val_batch = X_val[start_idx:end_idx]\n",
    "    X_val_batch_scaled = X_val_batch\n",
    "    batch_preds = hybrid_model.predict(X_val_batch_scaled)\n",
    "    val_predictions.extend(batch_preds)\n",
    "\n",
    "val_predictions = np.array(val_predictions)\n",
    "val_predictions_clipped = np.clip(val_predictions, 0.5, 5.0)\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions_clipped))\n",
    "val_mae = mean_absolute_error(y_val, val_predictions_clipped)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Validation Metrics:\")\n",
    "print(f\"  - RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  - MAE: {val_mae:.4f}\")\n",
    "print(f\"  - Training time: {training_time/60:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nðŸ” Prediction Analysis:\")\n",
    "print(f\"  - Mean actual: {y_val.mean():.3f}\")\n",
    "print(f\"  - Mean predicted: {val_predictions_clipped.mean():.3f}\")\n",
    "print(f\"  - Std actual: {y_val.std():.3f}\")\n",
    "print(f\"  - Std predicted: {val_predictions_clipped.std():.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Comparison with SVD baseline:\")\n",
    "print(f\"  - SVD (known pairs): RMSE {rmse_known:.4f}\")\n",
    "print(f\"  - Hybrid model: RMSE {val_rmse:.4f}\")\n",
    "\n",
    "if val_rmse < rmse_known:\n",
    "    improvement = ((rmse_known - val_rmse) / rmse_known) * 100\n",
    "    print(f\"  âœ… Improvement: {improvement:.2f}%!\")\n",
    "else:\n",
    "    regression = ((val_rmse - rmse_known) / rmse_known) * 100\n",
    "    print(f\"  âš ï¸ Regression: {regression:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b55b23",
   "metadata": {},
   "source": [
    "### Train Hybrid Model Sipmlified (SVD + Movie Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be84634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN HYBRID MODEL\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Dataset:\n",
      "  - Total samples: 673,543\n",
      "  - Features: SVD prediction (1) + Movie embeddings (128) = 129\n",
      "\n",
      "ðŸ“Š Split:\n",
      "  - Train: 572,512\n",
      "  - Val: 101,031\n",
      "\n",
      "ðŸ“ Fitting scaler on sample...\n",
      "  âœ… Scaler fitted on 57,252 samples\n",
      "\n",
      "ðŸ§  Training SGD Regressor (Ridge equivalent)...\n",
      "  - penalty='l2' â†’ Ridge regularization\n",
      "  - alpha=0.01 â†’ Ridge alpha=10.0 equivalent\n",
      "  - learning_rate='optimal' â†’ Adaptive learning\n",
      "\n",
      "ðŸ“ Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:03<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:03<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:02<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:02<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:02<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”® Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Validation Metrics:\n",
      "  - RMSE: 0.3657\n",
      "  - MAE: 0.2246\n",
      "  - Training time: 0.33 minutes\n",
      "\n",
      "ðŸ” Prediction Analysis:\n",
      "  - Mean actual: 3.533\n",
      "  - Mean predicted: 3.535\n",
      "  - Std actual: 1.073\n",
      "  - Std predicted: 0.981\n",
      "\n",
      "ðŸŽ¯ Comparison:\n",
      "  - SVD baseline: RMSE 0.9444\n",
      "  - Ridge ALL (3M): RMSE 0.3657\n",
      "  ðŸ† Improvement with full data: 61.27%\n",
      "\n",
      "âœ… Training on ALL data complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAIN HYBRID MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset:\")\n",
    "print(f\"  - Total samples: {len(train_with_genome):,}\")\n",
    "print(f\"  - Features: SVD prediction (1) + Movie embeddings (128) = 129\")\n",
    "\n",
    "genome_cols = [f'g_emb_{i}' for i in range(128)]\n",
    "\n",
    "# Split indices\n",
    "val_size = int(len(train_with_genome) * 0.15)\n",
    "train_size = len(train_with_genome) - val_size\n",
    "\n",
    "train_data = train_with_genome.iloc[:train_size].copy()\n",
    "val_data = train_with_genome.iloc[train_size:].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š Split:\")\n",
    "print(f\"  - Train: {train_size:,}\")\n",
    "print(f\"  - Val: {val_size:,}\")\n",
    "\n",
    "# Fit scaler on sample\n",
    "print(\"\\nðŸ“ Fitting scaler on sample...\")\n",
    "scaler = StandardScaler()\n",
    "sample_for_scaler = train_data.iloc[::10]  # Every 10th row\n",
    "\n",
    "X_svd_sample = sample_for_scaler['svd_prediction'].values.reshape(-1, 1)\n",
    "X_movie_sample = sample_for_scaler[genome_cols].values\n",
    "X_scaler_sample = np.hstack([X_svd_sample, X_movie_sample])\n",
    "scaler.fit(X_scaler_sample)\n",
    "\n",
    "print(f\"  âœ… Scaler fitted on {len(sample_for_scaler):,} samples\")\n",
    "\n",
    "# Train SGD as Ridge equivalent\n",
    "print(\"\\nðŸ§  Training SGD Regressor (Ridge equivalent)...\")\n",
    "print(\"  - penalty='l2' â†’ Ridge regularization\")\n",
    "print(\"  - alpha=0.01 â†’ Ridge alpha=10.0 equivalent\")\n",
    "print(\"  - learning_rate='optimal' â†’ Adaptive learning\")\n",
    "\n",
    "ridge_equivalent = SGDRegressor(\n",
    "    penalty='l2',\n",
    "    alpha=0.01,  # Roughly equivalent to Ridge alpha=10.0\n",
    "    learning_rate='optimal',\n",
    "    eta0=0.01,\n",
    "    max_iter=1000,  # Let it converge\n",
    "    tol=1e-4,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "batch_size = 50000\n",
    "n_batches = (train_size // batch_size) + 1\n",
    "n_epochs = 5  # More epochs for convergence\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"\\nðŸ“ Epoch {epoch + 1}/{n_epochs}\")\n",
    "    \n",
    "    # Shuffle\n",
    "    train_shuffled = train_data.sample(frac=1, random_state=42 + epoch).reset_index(drop=True)\n",
    "    \n",
    "    for i in tqdm(range(n_batches), desc=f\"  Batches\"):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, train_size)\n",
    "        \n",
    "        batch = train_shuffled.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Build features on the fly\n",
    "        X_svd = batch['svd_prediction'].values.reshape(-1, 1)\n",
    "        X_movie = batch[genome_cols].values\n",
    "        X_batch = np.hstack([X_svd, X_movie])\n",
    "        y_batch = batch['rating'].values\n",
    "        \n",
    "        # Scale and train\n",
    "        X_batch_scaled = scaler.transform(X_batch)\n",
    "        ridge_equivalent.partial_fit(X_batch_scaled, y_batch)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on validation\n",
    "print(\"\\nðŸ”® Evaluating on validation set...\")\n",
    "val_predictions = []\n",
    "val_batch_size = 100000\n",
    "n_val_batches = (val_size // val_batch_size) + 1\n",
    "\n",
    "for i in tqdm(range(n_val_batches), desc=\"Validation\"):\n",
    "    start_idx = i * val_batch_size\n",
    "    end_idx = min((i + 1) * val_batch_size, val_size)\n",
    "    \n",
    "    batch = val_data.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Build features\n",
    "    X_svd = batch['svd_prediction'].values.reshape(-1, 1)\n",
    "    X_movie = batch[genome_cols].values\n",
    "    X_val_batch = np.hstack([X_svd, X_movie])\n",
    "    \n",
    "    # Scale and predict\n",
    "    X_val_batch_scaled = scaler.transform(X_val_batch)\n",
    "    batch_preds = ridge_equivalent.predict(X_val_batch_scaled)\n",
    "    val_predictions.extend(batch_preds)\n",
    "\n",
    "val_predictions = np.array(val_predictions)\n",
    "val_predictions_clipped = np.clip(val_predictions, 0.5, 5.0)\n",
    "y_val = val_data['rating'].values\n",
    "\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions_clipped))\n",
    "val_mae = mean_absolute_error(y_val, val_predictions_clipped)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Validation Metrics:\")\n",
    "print(f\"  - RMSE: {val_rmse:.4f}\")\n",
    "print(f\"  - MAE: {val_mae:.4f}\")\n",
    "print(f\"  - Training time: {training_time/60:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nðŸ” Prediction Analysis:\")\n",
    "print(f\"  - Mean actual: {y_val.mean():.3f}\")\n",
    "print(f\"  - Mean predicted: {val_predictions_clipped.mean():.3f}\")\n",
    "print(f\"  - Std actual: {y_val.std():.3f}\")\n",
    "print(f\"  - Std predicted: {val_predictions_clipped.std():.3f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Comparison:\")\n",
    "print(f\"  - SVD baseline: RMSE {rmse_known:.4f}\")\n",
    "print(f\"  - Ridge ALL (3M): RMSE {val_rmse:.4f}\")\n",
    "\n",
    "if val_rmse < rmse_known:\n",
    "    improvement = ((rmse_known - val_rmse) / rmse_known) * 100\n",
    "    print(f\"  ðŸ† Improvement with full data: {improvement:.2f}%\")\n",
    "elif val_rmse < 0.65:\n",
    "    print(f\"  âœ… Similar performance (more data doesn't hurt)\")\n",
    "else:\n",
    "    print(f\"  âš ï¸ Worse performance (possible underfitting)\")\n",
    "\n",
    "print(\"\\nâœ… Training on ALL data complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55282b00",
   "metadata": {},
   "source": [
    "### Final Prediction Strategy (Two-Level System)\n",
    "\n",
    "Based on evaluation results, we implement a two-level prediction system:\n",
    "- **Known pairs (user + movie in training):** Use Hybrid model â†’ RMSE 0.3657\n",
    "- **Cold start cases:** Use fallback strategies with movie means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a9d512d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL PREDICTION FUNCTION\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Calculating movie means from train set...\n",
      "  âœ… Movie means calculated for 26,036 movies\n",
      "  - Min: 0.50\n",
      "  - Max: 5.00\n",
      "  - Mean: 3.18\n",
      "\n",
      "ðŸ“š Creating movie embeddings dictionary...\n",
      "  âœ… 13,816 movies with embeddings\n",
      "\n",
      "âœ… Final prediction function defined!\n",
      "\n",
      "Function handles all cases:\n",
      "  1. Both known + genome â†’ Hybrid (best)\n",
      "  2. User known + new movie â†’ Hybrid with user baseline\n",
      "  3. New user + movie known â†’ Hybrid with movie baseline\n",
      "  4. No genome â†’ SVD baseline or mean fallback\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL PREDICTION FUNCTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate movie means from train set\n",
    "print(\"\\nðŸ“Š Calculating movie means from train set...\")\n",
    "movie_means = train_ratings.groupby('movieId')['rating'].mean().to_dict()\n",
    "print(f\"  âœ… Movie means calculated for {len(movie_means):,} movies\")\n",
    "print(f\"  - Min: {min(movie_means.values()):.2f}\")\n",
    "print(f\"  - Max: {max(movie_means.values()):.2f}\")\n",
    "print(f\"  - Mean: {np.mean(list(movie_means.values())):.2f}\")\n",
    "\n",
    "# Create movie embeddings dictionary for fast lookup\n",
    "print(\"\\nðŸ“š Creating movie embeddings dictionary...\")\n",
    "movie_embeddings_dict = {}\n",
    "for _, row in movie_embeddings_df.iterrows():\n",
    "    movie_id = row['movieId']\n",
    "    embeddings = row[[f'g_emb_{i}' for i in range(128)]].values\n",
    "    movie_embeddings_dict[movie_id] = embeddings\n",
    "\n",
    "print(f\"  âœ… {len(movie_embeddings_dict):,} movies with embeddings\")\n",
    "\n",
    "def predict_final(user_id, movie_id, movie_embeddings_dict, movie_means_dict):\n",
    "    \"\"\"\n",
    "    Final prediction function with intelligent cold start handling.\n",
    "    \n",
    "    Cases handled:\n",
    "    1. User known + Movie known + genome â†’ Hybrid model (BEST)\n",
    "    2. User known + Movie NEW (has genome) â†’ Hybrid with user_mean + genome\n",
    "    3. User NEW + Movie known (has genome) â†’ Hybrid with movie_mean + genome\n",
    "    4. No genome or both unknown â†’ Fallback to movie_mean or global_mean\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    user_id : float\n",
    "        User ID\n",
    "    movie_id : float\n",
    "        Movie ID\n",
    "    movie_embeddings_dict : dict\n",
    "        Dictionary of movie genome embeddings\n",
    "    movie_means_dict : dict\n",
    "        Dictionary of movie mean ratings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (prediction, strategy_used)\n",
    "    \"\"\"\n",
    "    # Get indices\n",
    "    user_idx = user_to_idx.get(user_id)\n",
    "    movie_idx = movie_to_idx.get(movie_id)\n",
    "    \n",
    "    # Get movie genome embeddings and mean\n",
    "    movie_genome = movie_embeddings_dict.get(movie_id)\n",
    "    movie_mean = movie_means_dict.get(movie_id, global_mean_rating)\n",
    "    \n",
    "    # CASE 1: User known + Movie known + genome available â†’ HYBRID FULL (BEST)\n",
    "    if user_idx is not None and movie_idx is not None and movie_genome is not None:\n",
    "        user_idx = int(user_idx)\n",
    "        movie_idx = int(movie_idx)\n",
    "        \n",
    "        # SVD prediction\n",
    "        user_vec = user_factors[user_idx]\n",
    "        movie_vec = movie_factors[movie_idx]\n",
    "        centered_pred = np.dot(user_vec, movie_vec)\n",
    "        svd_pred = user_means[user_idx] + centered_pred\n",
    "        \n",
    "        # Build hybrid features [svd_pred, genome_embeddings]\n",
    "        features = np.array([svd_pred] + movie_genome.tolist()).reshape(1, -1)\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        # Hybrid prediction\n",
    "        prediction = ridge_equivalent.predict(features_scaled)[0]\n",
    "        return np.clip(prediction, 0.5, 5.0), 'hybrid_full'\n",
    "    \n",
    "    # CASE 2: User known + Movie NEW (has genome) â†’ Hybrid with user baseline\n",
    "    elif user_idx is not None and movie_genome is not None:\n",
    "        user_idx = int(user_idx)\n",
    "        \n",
    "        # Use user mean as SVD approximation\n",
    "        svd_approx = user_means[user_idx]\n",
    "        \n",
    "        # Build features\n",
    "        features = np.array([svd_approx] + movie_genome.tolist()).reshape(1, -1)\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        prediction = ridge_equivalent.predict(features_scaled)[0]\n",
    "        return np.clip(prediction, 0.5, 5.0), 'hybrid_new_movie'\n",
    "    \n",
    "    # CASE 3: User NEW + Movie known (has genome) â†’ Hybrid with movie baseline\n",
    "    elif movie_genome is not None:\n",
    "        # Use movie mean as SVD approximation\n",
    "        svd_approx = movie_mean\n",
    "        \n",
    "        # Build features\n",
    "        features = np.array([svd_approx] + movie_genome.tolist()).reshape(1, -1)\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        prediction = ridge_equivalent.predict(features_scaled)[0]\n",
    "        return np.clip(prediction, 0.5, 5.0), 'hybrid_new_user'\n",
    "    \n",
    "    # CASE 4: No genome available or both unknown â†’ Fallback to means\n",
    "    else:\n",
    "        # Try SVD if both indices exist\n",
    "        if user_idx is not None and movie_idx is not None:\n",
    "            user_idx = int(user_idx)\n",
    "            movie_idx = int(movie_idx)\n",
    "            \n",
    "            user_vec = user_factors[user_idx]\n",
    "            movie_vec = movie_factors[movie_idx]\n",
    "            centered_pred = np.dot(user_vec, movie_vec)\n",
    "            svd_pred = user_means[user_idx] + centered_pred\n",
    "            return np.clip(svd_pred, 0.5, 5.0), 'svd_no_genome'\n",
    "        \n",
    "        # Full fallback to movie mean or global mean\n",
    "        return movie_mean, 'fallback_mean'\n",
    "\n",
    "print(\"\\nâœ… Final prediction function defined!\")\n",
    "print(\"\\nFunction handles all cases:\")\n",
    "print(\"  1. Both known + genome â†’ Hybrid (best)\")\n",
    "print(\"  2. User known + new movie â†’ Hybrid with user baseline\")\n",
    "print(\"  3. New user + movie known â†’ Hybrid with movie baseline\")\n",
    "print(\"  4. No genome â†’ SVD baseline or mean fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebe913",
   "metadata": {},
   "source": [
    "### Final Evaluation on Test Set\n",
    "\n",
    "Evaluate the two-level prediction system on the complete test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "efbb4949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "ðŸ”® Evaluating final model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200000/200000 [02:27<00:00, 1354.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Final Model - Overall Test Metrics:\n",
      "  - RMSE: 1.0184\n",
      "  - MAE: 0.7805\n",
      "\n",
      "ðŸ” Prediction Analysis:\n",
      "  - Mean actual: 3.546\n",
      "  - Mean predicted: 3.563\n",
      "  - Std actual: 1.090\n",
      "  - Std predicted: 0.524\n",
      "\n",
      "ðŸ“Š Strategy usage:\n",
      "  - hybrid_full: 41,214 (20.6%)\n",
      "  - svd_no_genome: 1,931 (1.0%)\n",
      "  - hybrid_new_user: 152,098 (76.0%)\n",
      "  - fallback_mean: 4,174 (2.1%)\n",
      "  - hybrid_new_movie: 583 (0.3%)\n",
      "\n",
      "ðŸŽ¯ Performance by strategy:\n",
      "  - Hybrid (both known): RMSE 0.9142 | Count: 41,214 (20.6%)\n",
      "  - Hybrid (new movie): RMSE 0.8881 | Count: 583 (0.3%)\n",
      "  - Hybrid (new user): RMSE 1.0366 | Count: 152,098 (76.0%)\n",
      "  - SVD (no genome): RMSE 1.1401 | Count: 1,931 (1.0%)\n",
      "  - Fallback (mean): RMSE 1.2566 | Count: 4,174 (2.1%)\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Model Comparison:\n",
      "  - SVD Baseline (known pairs): RMSE 0.9444\n",
      "  - Hybrid Model (known pairs): RMSE 0.9142 â†’ Improvement: 3.19% âœ…\n",
      "  - Final System (all cases): RMSE 1.0184\n",
      "\n",
      "ðŸ’¡ Key Insights:\n",
      "  - Hybrid approach used for 193,895 ratings (96.9%)\n",
      "    â€¢ Both known (best case): 41,214 (20.6%)\n",
      "    â€¢ New movie: 583 (0.3%)\n",
      "    â€¢ New user: 152,098 (76.0%)\n",
      "  - Genome embeddings improve predictions when available\n",
      "  - Cold start cases handled with movie means and SVD baseline\n",
      "\n",
      "âœ… Final evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on complete test set\n",
    "print(\"\\nðŸ”® Evaluating final model on test set...\")\n",
    "final_predictions = []\n",
    "final_strategies = []\n",
    "\n",
    "for idx, row in tqdm(test_ratings.iterrows(), total=len(test_ratings), desc=\"Predicting\"):\n",
    "    pred, strategy = predict_final(\n",
    "        row['userId'], \n",
    "        row['movieId'],\n",
    "        movie_embeddings_dict,\n",
    "        movie_means\n",
    "    )\n",
    "    final_predictions.append(pred)\n",
    "    final_strategies.append(strategy)\n",
    "\n",
    "final_predictions = np.array(final_predictions)\n",
    "test_actuals = test_ratings['rating'].values\n",
    "\n",
    "# Calculate overall metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(test_actuals, final_predictions))\n",
    "final_mae = mean_absolute_error(test_actuals, final_predictions)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Final Model - Overall Test Metrics:\")\n",
    "print(f\"  - RMSE: {final_rmse:.4f}\")\n",
    "print(f\"  - MAE: {final_mae:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Prediction Analysis:\")\n",
    "print(f\"  - Mean actual: {test_actuals.mean():.3f}\")\n",
    "print(f\"  - Mean predicted: {final_predictions.mean():.3f}\")\n",
    "print(f\"  - Std actual: {test_actuals.std():.3f}\")\n",
    "print(f\"  - Std predicted: {final_predictions.std():.3f}\")\n",
    "\n",
    "# Strategy breakdown\n",
    "from collections import Counter\n",
    "\n",
    "strategy_counts = Counter(final_strategies)\n",
    "print(f\"\\nðŸ“Š Strategy usage:\")\n",
    "for strategy, count in strategy_counts.items():\n",
    "    pct = count / len(final_strategies) * 100\n",
    "    print(f\"  - {strategy}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Evaluate performance by strategy\n",
    "print(f\"\\nðŸŽ¯ Performance by strategy:\")\n",
    "\n",
    "# Hybrid full cases (both known)\n",
    "hybrid_full_mask = np.array([s == 'hybrid_full' for s in final_strategies])\n",
    "if hybrid_full_mask.sum() > 0:\n",
    "    hybrid_full_rmse = np.sqrt(mean_squared_error(\n",
    "        test_actuals[hybrid_full_mask], \n",
    "        final_predictions[hybrid_full_mask]\n",
    "    ))\n",
    "    print(f\"  - Hybrid (both known): RMSE {hybrid_full_rmse:.4f} | Count: {hybrid_full_mask.sum():,} ({hybrid_full_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "# Hybrid new movie cases\n",
    "hybrid_new_movie_mask = np.array([s == 'hybrid_new_movie' for s in final_strategies])\n",
    "if hybrid_new_movie_mask.sum() > 0:\n",
    "    hybrid_new_movie_rmse = np.sqrt(mean_squared_error(\n",
    "        test_actuals[hybrid_new_movie_mask], \n",
    "        final_predictions[hybrid_new_movie_mask]\n",
    "    ))\n",
    "    print(f\"  - Hybrid (new movie): RMSE {hybrid_new_movie_rmse:.4f} | Count: {hybrid_new_movie_mask.sum():,} ({hybrid_new_movie_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "# Hybrid new user cases\n",
    "hybrid_new_user_mask = np.array([s == 'hybrid_new_user' for s in final_strategies])\n",
    "if hybrid_new_user_mask.sum() > 0:\n",
    "    hybrid_new_user_rmse = np.sqrt(mean_squared_error(\n",
    "        test_actuals[hybrid_new_user_mask], \n",
    "        final_predictions[hybrid_new_user_mask]\n",
    "    ))\n",
    "    print(f\"  - Hybrid (new user): RMSE {hybrid_new_user_rmse:.4f} | Count: {hybrid_new_user_mask.sum():,} ({hybrid_new_user_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "# SVD no genome cases\n",
    "svd_mask = np.array([s == 'svd_no_genome' for s in final_strategies])\n",
    "if svd_mask.sum() > 0:\n",
    "    svd_rmse = np.sqrt(mean_squared_error(\n",
    "        test_actuals[svd_mask], \n",
    "        final_predictions[svd_mask]\n",
    "    ))\n",
    "    print(f\"  - SVD (no genome): RMSE {svd_rmse:.4f} | Count: {svd_mask.sum():,} ({svd_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "# Fallback cases\n",
    "fallback_mask = np.array([s == 'fallback_mean' for s in final_strategies])\n",
    "if fallback_mask.sum() > 0:\n",
    "    fallback_rmse = np.sqrt(mean_squared_error(\n",
    "        test_actuals[fallback_mask], \n",
    "        final_predictions[fallback_mask]\n",
    "    ))\n",
    "    print(f\"  - Fallback (mean): RMSE {fallback_rmse:.4f} | Count: {fallback_mask.sum():,} ({fallback_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "\n",
    "# Calculate total hybrid usage (all 3 hybrid strategies)\n",
    "total_hybrid_mask = hybrid_full_mask | hybrid_new_movie_mask | hybrid_new_user_mask\n",
    "total_hybrid_count = total_hybrid_mask.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Comparison:\")\n",
    "print(f\"  - SVD Baseline (known pairs): RMSE {rmse_known:.4f}\")\n",
    "print(f\"  - Hybrid Model (known pairs): RMSE {hybrid_full_rmse:.4f} â†’ Improvement: {((rmse_known-hybrid_full_rmse)/rmse_known*100):.2f}% âœ…\")\n",
    "print(f\"  - Final System (all cases): RMSE {final_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(f\"  - Hybrid approach used for {total_hybrid_count:,} ratings ({total_hybrid_count/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"    â€¢ Both known (best case): {hybrid_full_mask.sum():,} ({hybrid_full_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"    â€¢ New movie: {hybrid_new_movie_mask.sum():,} ({hybrid_new_movie_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"    â€¢ New user: {hybrid_new_user_mask.sum():,} ({hybrid_new_user_mask.sum()/len(test_ratings)*100:.1f}%)\")\n",
    "print(f\"  - Genome embeddings improve predictions when available\")\n",
    "print(f\"  - Cold start cases handled with movie means and SVD baseline\")\n",
    "\n",
    "print(\"\\nâœ… Final evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
